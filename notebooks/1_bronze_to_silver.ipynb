{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97fed643-41c9-406d-9974-fbb86f57dfa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Mounting the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34e56da2-93da-4c85-a394-1b97cfae766f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "storage_account_name = \"STORAGE ACCOUNT NAME\" # <--- UPDATE THIS\n",
    "storage_account_key = \"KEY\" # <--- UPDATE THIS\n",
    "container_name = \"bronze\"\n",
    "\n",
    "# Create the specific URL for mounting\n",
    "mount_point = f\"/mnt/{container_name}\"\n",
    "source_url = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\"\n",
    "extra_configs = {f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": storage_account_key}\n",
    "\n",
    "# Check if already mounted to avoid errors\n",
    "if any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "    print(f\"{mount_point} is already mounted.\")\n",
    "else:\n",
    "    # Mount the drive\n",
    "    dbutils.fs.mount(\n",
    "        source=source_url,\n",
    "        mount_point=mount_point,\n",
    "        extra_configs=extra_configs\n",
    "    )\n",
    "    print(f\"Mounted {mount_point} successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41c64d46-92d1-49fd-9582-34c5492894ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %fs ls /mnt/bronze/raw_files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d14a9d2-5e56-470f-a8a8-8bae6290b834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "source_path = \"/mnt/bronze/raw_files/\"\n",
    "\n",
    "# 1. Get the list of file paths\n",
    "file_paths = [file.path for file in dbutils.fs.ls(source_path)]\n",
    "\n",
    "# 2. Define an empty DataFrame to start\n",
    "final_df = None\n",
    "\n",
    "# 3. Loop through every file\n",
    "for path in file_paths:\n",
    "    # Read the individual file\n",
    "    # We allow the \"Vectorized Reader\" to be off to handle small type changes\n",
    "    spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "    \n",
    "    current_df = spark.read.parquet(path)\n",
    "    \n",
    "    # If this is the first file, make it the \"Master\" DataFrame\n",
    "    if final_df is None:\n",
    "        final_df = current_df\n",
    "    else:\n",
    "        # 4. The Magic Trick: unionByName with allowMissingColumns=True\n",
    "        # This merges them even if schemas are slightly different\n",
    "        final_df = final_df.unionByName(current_df, allowMissingColumns=True)\n",
    "\n",
    "# 5. Display the result\n",
    "print(\"Success! All files merged.\")\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6b7b59-4a0f-45a0-ac32-1e221b5daddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # to check the df has all 12 months\n",
    "# from pyspark.sql.functions import col, month\n",
    "\n",
    "# # Get a list of unique months present in the data\n",
    "# present_months = [row['month'] for row in final_df.select(month(col(\"tpep_pickup_datetime\")).alias(\"month\")).distinct().collect()]\n",
    "\n",
    "# # Compare against the full set of months (1 to 12)\n",
    "# all_months = set(range(1, 13))\n",
    "# print(all_months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da51041e-5ed0-48ee-ab79-c292b80f82fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver layer transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "474a22be-ea08-4cde-b5cc-6721a2964e7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration for Silver\n",
    "container_name = \"silver\"\n",
    "storage_account_name = \"stgadlsurban\" # Put your storage account name here\n",
    "storage_account_key = \"AmkqdoobmkU45h233gmEtdCIVQJVzu434cRSfjmlrJKhUGzR1F4+5Scp9hHgl6Rst45+C0CpI9/5+AStlsmzYg==\" # access_key\n",
    "\n",
    "source_url = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\"\n",
    "mount_point = f\"/mnt/{container_name}\"\n",
    "extra_configs = {f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": storage_account_key}\n",
    "\n",
    "# Mount Silver if not already mounted\n",
    "if not any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "    dbutils.fs.mount(\n",
    "        source=source_url,\n",
    "        mount_point=mount_point,\n",
    "        extra_configs=extra_configs\n",
    "    )\n",
    "    print(f\"Mounted {mount_point} successfully!\")\n",
    "else:\n",
    "    print(f\"{mount_point} is already mounted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e0d6960-1a9f-4a23-8ba2-21711ae6756d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765991121856}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, current_timestamp\n",
    "\n",
    "# 1. Select and Rename columns for clarity\n",
    "# We are creating a new DataFrame 'silver_df' with clean types\n",
    "silver_df = final_df.select(\n",
    "    col(\"VendorID\").cast(\"long\").alias(\"vendor_id\"),\n",
    "    to_timestamp(col(\"tpep_pickup_datetime\")).alias(\"pickup_time\"),\n",
    "    to_timestamp(col(\"tpep_dropoff_datetime\")).alias(\"dropoff_time\"),\n",
    "    col(\"passenger_count\").cast(\"int\"),\n",
    "    col(\"trip_distance\").cast(\"double\"),\n",
    "    col(\"fare_amount\").cast(\"double\"),\n",
    "    col(\"total_amount\").cast(\"double\"),\n",
    "    col(\"payment_type\").cast(\"long\"),\n",
    "    col(\"PULocationID\").cast(\"long\"),\n",
    "    col(\"DOLocationID\").cast(\"long\"),\n",
    "    col(\"RatecodeID\").cast(\"long\")\n",
    ")\n",
    "\n",
    "# 2. Apply Filters (Business Logic)\n",
    "# Remove rows where trip_distance is zero or fare is negative (bad data)\n",
    "silver_df_clean = silver_df.filter(\n",
    "    (col(\"trip_distance\") > 0) & \n",
    "    (col(\"total_amount\") > 0)\n",
    ")\n",
    "\n",
    "# 3. Add an \"Ingestion Date\" (Audit Trail)\n",
    "silver_df_clean = silver_df_clean.withColumn(\"ingestion_date\", current_timestamp())\n",
    "\n",
    "# Verify the clean data\n",
    "print(\"Silver Data Preview:\")\n",
    "display(silver_df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc12404a-e387-4543-b321-660a4f8eafa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the clean data to the Silver layer in Delta format\n",
    "silver_path = \"/mnt/silver/taxi_trips\"\n",
    "\n",
    "silver_df_clean.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(silver_path)\n",
    "\n",
    "print(f\"Successfully wrote Delta Table to {silver_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3a03205-3371-4560-b200-ef04f4f3cab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /mnt/silver/taxi_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be7dda5b-660c-4f53-94da-cf6cf45e105d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Download the CSV file from the web to the driver's temp folder\n",
    "import os\n",
    "\n",
    "# Define the URL and where we want to save it locally on the cluster\n",
    "url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\"\n",
    "local_path = \"/tmp/taxi_zone_lookup.csv\"\n",
    "\n",
    "# Use wget to download (Linux command)\n",
    "os.system(f\"wget {url} -O {local_path}\")\n",
    "\n",
    "# Step 2: Read the downloaded CSV into Spark\n",
    "# We use \"file://\" to tell Spark to look at the local hard drive\n",
    "zone_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(f\"file://{local_path}\")\n",
    "\n",
    "# Step 3: Save it as a Delta Table in your Silver layer\n",
    "zone_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/mnt/silver/taxi_zone_lookup\")\n",
    "\n",
    "# Step 4: Register it as a SQL table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS urban_mobility.taxi_zone_lookup\n",
    "    USING DELTA\n",
    "    LOCATION '/mnt/silver/taxi_zone_lookup'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Success! Taxi Zone Lookup table created.\")\n",
    "display(zone_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4185dd-f6a2-4024-8017-5763083703a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Create a Database (Schema) to organize our tables\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS urban_mobility\")\n",
    "\n",
    "# 2. Register the Silver Delta files as a Table\n",
    "# This doesn't move data; it just points to your existing Silver folder.\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS urban_mobility.silver_taxi\n",
    "    USING DELTA\n",
    "    LOCATION '/mnt/silver/taxi_trips'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Table 'urban_mobility.silver_taxi' created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb6a5296-5a3c-4262-b885-6de1f408bd0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM urban_mobility.silver_taxi LIMIT 10"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7222450654316421,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1_bronze_to_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
